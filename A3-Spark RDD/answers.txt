1. What was wrong with the original wordcount-5 data set that made repartitioning worth it? Why did the program run faster after?

Time Count:

with repartition

real    2m7.860s
user    0m26.753s
sys     0m2.956s

without 

real    4m28.099s
user    0m27.133s
sys     0m2.

We can take a look at the datasets firstly.
xxxx@pmp-gateway:~$ hdfs dfs -ls /courses/732/wordcount-5
Found 8 items
-rw-r--r--   3 ggbaker supergroup  272333102 2020-12-16 09:13 /courses/732/wordcount-5/7.txt.gz
-rw-r--r--   3 ggbaker supergroup      93397 2020-12-16 09:03 /courses/732/wordcount-5/F.txt.gz
-rw-r--r--   3 ggbaker supergroup      84223 2020-12-16 09:58 /courses/732/wordcount-5/S.txt.gz
-rw-r--r--   3 ggbaker supergroup   44054520 2020-12-16 09:57 /courses/732/wordcount-5/d.txt.gz
-rw-r--r--   3 ggbaker supergroup   93890605 2020-12-16 09:32 /courses/732/wordcount-5/g.txt.gz
-rw-r--r--   3 ggbaker supergroup  116015482 2020-12-16 09:41 /courses/732/wordcount-5/m.txt.gz
-rw-r--r--   3 ggbaker supergroup   19394837 2020-12-16 09:30 /courses/732/wordcount-5/o.txt.gz
-rw-r--r--   3 ggbaker supergroup   79300825 2020-12-16 10:00 /courses/732/wordcount-5/s.txt.gz

Before repartitioning, some executors had to process larger datasets than others, which resulted in the total processing time added.
After repartitioning, executors could process similar size of datasets in parallel, so that the processing time is shorter than the one without repartitioning.


2.The same fix does not make this code run faster on the wordcount-3 data set. (It may be slightly slower?) Why? [For once, the answer is not “the data set is too small”.]

The processing time of repartitioning fix on the wordcount-3 is slightly lower than the one without partitioning.
The size of each dataset is very similar to each other in the wordcount-3.
Therefore, even if the repartition method was added, there was not much difference between the workload of each executor.

3.How could you modify the wordcount-5 input so that the word count code can process it and get the same results as fast as possible? (It's possible to get about another minute off the running time.)

I think re-size the datasets would be better, like reducing the larger datasets to smaller ones. 
The processing time of each executor was largely affected by different size of the datasets, so the most effective way is to modify the size of datasets before processing them.

4.When experimenting with the number of partitions while estimating Euler's constant, you likely didn't see much difference for a range of values, and chose the final value in your code somewhere in that range. What range of partitions numbers was “good” (on the desktop/laptop where you were testing)?

I believe there is a 'good' range about the partitions, but I am still working on it. Appoximately it's (100, 200) on my laptop.

